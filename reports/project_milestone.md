# Camera-only 3D Semantic Occupancy (OccFormer on Waymo/nuScenes)
Su Hyun Kim (018219422) 

## Achievements so far
- NuScenes occupancy evaluation completed (OccFormer baseline); metrics logged in `logs/nusc_infos.11975.out`. Current numbers are modest, highlighting the need for tuning; they serve as a baseline/zero-shot reference.
- Waymo data fully preprocessed into trainable form: Occ3D raw labels (15 classes + free) downloaded via `tools/data_downloader/waymo`, converted to KITTI-format infos/poses, and stored in `data/waymo_v1-3-1/occ3d_waymo`; code for processing, training, evaluation, and debugging is in place.
- Waymo Occ3D training/eval runs end-to-end. `baseline_fast` (1/10 sampling, 30 epochs) trains and evaluates without crashes; checkpoints/logs under `results/baseline_fast`. Evaluation prints SC IoU / SSC mIoU and per-class IoU tables (percent) in `results/baseline_fast/logs/evaluate.log`.
- Environment hurdles on P100 were resolved by using the local CUDA 11.3 extraction with a conda gcc/g++ 9 toolchain, an nvcc wrapper to bypass compiler checks, explicit CPATH/LD_LIBRARY_PATH for cudart/cublas/cusparse/cupti, and `TORCH_CUDA_ARCH_LIST=6.0+PTX`, and the node-specific setup is documented in `docs/installation.md` and `docs/environment_setting.md`.

## Directory structure (key folders)
```
OccFormerWithWaymoData/
├── projects/
│   ├── configs/occformer_waymo/      # Base config and experiment overrides (baseline_fast, strong_aug, efficient_b4, sgd)
│   └── mmdet3d_plugin/
│       ├── datasets/                 # Waymo dataset impls, occupancy metrics, loaders
│       ├── occformer/                # Detectors, heads, image2bev ops (ViewTransformerLSSVoxel)
│       └── utils/                    # SSC metrics and helpers
├── tools/                            # Train/test entrypoints, data converters, downloaders
│   └── data_downloader/waymo/        # Occ3D Waymo download/prepare scripts
├── scripts/                          # SLURM runner and progress helpers
├── data/waymo_v1-3-1/occ3d_waymo/    # KITTI-format infos/poses/GT (15 classes + free)
├── results/                          # Experiment outputs (checkpoints/logs per exp)
├── mmdetection3d/                    # Forked mmdet3d with custom CUDA ops
└── docs/                             # Installation/env notes and guides
```
- Waymo-specific additions: `CustomWaymoDataset_T` (temporal, pose-aware, 16 classes incl. free), occupancy metrics (`occ_metrics.py`, `ssc_metric.py`), experiment overrides (`experiments.py`), and evaluation helpers that unwrap outputs, normalize voxel shapes, and print IoU tables.
- Data workflow: Occ3D Waymo GT (via `tools/data_downloader/waymo`), labels remapped to 15+free and stored as KITTI-format infos/poses in `data/waymo_v1-3-1/occ3d_waymo`; fast runs use ~10% data (`load_interval=10`) for iteration speed.

## Baseline modules
The baseline uses OccFormer configured for Waymo Occ3D: a camera-only occupancy model that lifts multi-view RGB into a voxel grid, performs 3D reasoning with deformable attention, and decodes semantic occupancy with a Mask2Former-style head. The system consumes five Waymo cameras (KITTI-format) with intrinsics/extrinsics; depth comes from LiDAR, and labels are the Occ3D remapped set (15 semantics + free) stored as voxel GT.

- Data flow and preprocessing
  - Images are loaded via `LoadMultiViewImageFromFiles_SemanticKitti`, depth supervision is generated by `CreateDepthFromLiDAR`, and occupancy GT is loaded by `LoadSemKittiAnnotation` with `occ_size=[256,256,32]`.
  - All labels are obtained from Occ3D Waymo and stored as KITTI-format infos/poses in `data/waymo_v1-3-1/occ3d_waymo`.
- Model architecture (Waymo base, `waymo_base.py`)
  - EfficientNet-B7 backbone + SECONDFPN neck produce multi-scale camera features. 
  - `ViewTransformerLiftSplatShootVoxel` lifts features to a voxel grid defined by `grid_config`. A 3D encoder (`OccupancyEncoder` + `MSDeformAttnPixelDecoder3D`) performs multi-scale 3D reasoning.
  - `Mask2FormerOccHead` (100 queries) outputs occupancy logits and a `count_matrix` confusion matrix for IoU computation.
- Training setup
  - `baseline_fast` runs 30 epochs with LR=1e-4, no validation during training, and stores outputs under `results/baseline_fast/model`. 
  - Image/BDA augmentations use the settings defined in `data_config` and `bda_aug_conf`.
  - The head optimizes cross-entropy and dice-style occupancy losses with a step LR schedule; optimizer choice (AdamW vs. SGD) is set in the experiment overrides.
- Evaluation
  - `tools/test.py` computes SC IoU (scene completion), SSC mIoU (semantic scene completion), and per-class IoU
  - Per-class IoUs are printed as percentages (e.g., 92.02 ⇒ IoU=0.9202), and their mean gives the final mIoU. Metrics are printed to the console/log at `results/<exp>/logs/evaluate.log`.
  - Example of per-class IoU result.
```
|        class         |  IoU  |
+----------------------+-------+
|       barrier        | 0.709 |
|       bicycle        | 0.401 |
|         bus          | 0.904 |
|         car          | 0.825 |
| construction_vehicle | 0.474 |
|      motorcycle      | 0.611 |
|      pedestrian      | 0.631 |
|     traffic_cone     | 0.419 |
|       trailer        |  0.6  |
|        truck         | 0.821 |
|  driveable_surface   | 0.931 |
|      other_flat      | 0.667 |
|       sidewalk       | 0.722 |
|       terrain        | 0.728 |
|       manmade        | 0.827 |
|      vegetation      | 0.798 |
|         mean         | 0.692 |
+----------------------+-------+
```

## Challenges and mitigation
- Old CUDA toolchain on P100 was a headache: getting CUDA 11.3 to play nicely with the current HPC setup required a local extraction, conda gcc9, an nvcc wrapper, explicit include/lib paths, and `TORCH_CUDA_ARCH_LIST=6.0+PTX` before mmdet3d would build. Rebuilding clean after clearing torch_extensions cache and keeping node-specific envs made it stable.
- Waymo scale vs. resources: the dataset is huge, sanity-checking it is painful, and full training is both memory- and time-hungry. To move fast we stick to ~10% sampling (`load_interval=10`), which speeds runs but makes convergence/quality harder; better scaling or smarter subset strategies are still needed.

## Milestone
- Run new experiments targeting augmentation/backbone/optimizer changes:
  - `strong_aug`: heavier bda/image augmentations for robustness.
  - `efficient_b4`: smaller EfficientNet-B4 to test capacity vs. speed/memory.
  - `sgd`: optimizer swap to SGD to compare convergence vs. AdamW/baseline.
- Diagnose current low performance: dig into per-class IoU, sampling (~10% subset), and potential label/coverage gaps; adjust data usage and training schedule accordingly.
- Visualization: add val prediction saving/visualization after new runs to inspect qualitative outputs.

## References
- [OccFormer](https://github.com/zhangyp15/OccFormer/tree/main) (ICCV’23): camera-only 3D semantic occupancy with dual-path transformers and Mask2Former-style decoder.
- [Occ3D](https://github.com/Tsinghua-MARS-Lab/Occ3D?tab=readme-ov-file) (Waymo occupancy labels/ontology): 15 semantic classes + free; KITTI-format infos/poses.
- [CVT-Occ](https://github.com/Tsinghua-MARS-Lab/CVT-Occ/tree/main) (multi-view occupancy work informing grid/lifting choices).
- [mmdetection3d](https://github.com/open-mmlab/mmdetection3d) as base framework.
