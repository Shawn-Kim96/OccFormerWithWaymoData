\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em
}

\title{Camera-only 3D Semantic Occupancy on Waymo with OccFormer}
\author{Su Hyun Kim (SJSU ID: 018219422)}
\date{December 2025}

\begin{document}
\maketitle

\section*{Cover Information}
\begin{tabular}{@{}ll@{}}
\toprule
Team ID & \textit{(fill in)} \\
Project Title & Camera-only 3D Semantic Occupancy on Waymo with OccFormer \\
Team Members & Su Hyun Kim (018219422), \textit{(email: fill in)} \\
Project Track & Fine-Tune \& Train (Option 2) \\
Focused Areas & 3D semantic occupancy, camera-only BEV/3D transformers, Waymo integration, debugging/memory \\
\bottomrule
\end{tabular}

\section*{Abstract}
This project began as a fork of the public OccFormer codebase and ended as a working Waymo occupancy pipeline inside MMDetection3D: dataset loading, training/evaluation scripts, and a qualitative video demo.
Most of the work was integration and debugging rather than changing the core model. The main friction points were aligning Waymo KITTI-format sensor exports with Occ3D-Waymo occupancy labels, fixing label conventions (free-space label), dealing with the fact that Waymo GT volumes are commonly 200$\times$200$\times$16 while the default model grid is 256$\times$256$\times$32, and keeping training stable on a single 12\,GB GPU.
The best run that completed end-to-end in this repo is \texttt{baseline\_fast} (10\% data via \texttt{load\_interval=10}, EfficientNet-B7, CrossEntropy), which reached 13.41 mIoU on the validation set based on the printed IoU table in \texttt{results/baseline\_fast/logs/evaluate.log}.
I also tried stronger augmentation, learning-rate changes, SGD, and multiple imbalance variants (focal loss, weighted CE, fewer Mask2Former queries). Several runs failed for concrete engineering reasons (registry import order, wrong image loader, SLURM environment issues, and CUDA out-of-memory), and this report documents those failures with log excerpts and the specific files I changed.

\section{Introduction \& Problem Description}
Semantic occupancy represents a driving scene as a dense 3D voxel grid around the ego vehicle, where each voxel is assigned an occupancy/semantic label. Compared to box-based perception, occupancy is useful for planning because it captures free space and structure even when objects are partially occluded.

My goal in this project was to run OccFormer on Waymo occupancy data and produce a submission-quality package: reproducible training and evaluation commands, quantitative results (tables and figures), and a short qualitative demo video that shows camera inputs and predicted BEV occupancy slices.
I selected Option 2 (fine-tune/train) because the main learning outcome for me was understanding the model and then making it actually run on a new dataset under real cluster constraints.

\section{Background / Related Work}
OccFormer~\cite{occformer} is a camera-only 3D semantic occupancy model built around a ``lift to 3D, then reason in 3D'' design.
It starts from multi-view camera images and extracts per-view features using a standard image backbone and neck.
To obtain 3D structure without LiDAR at inference time, it estimates depth distributions and uses a Lift-Splat-Shoot style view transformation to project image features into a voxel-aligned 3D volume in the ego coordinate system.
This is the part that connects 2D pixels to 3D voxels and makes the rest of the model ``geometry aware'' even though it is camera-only.

The 3D encoder in OccFormer is designed to be efficient on large voxel grids. Instead of applying fully dense attention everywhere, it splits the transformer encoder into two complementary pathways: a local pathway that attends within local 3D windows (good for fine structure) and a global pathway that uses a BEV-style representation to propagate long-range context.
The decoder adapts the Mask2Former idea to 3D occupancy: a fixed set of queries predicts class scores and mask embeddings, which are combined with voxel features to produce voxel-level logits. This query-based decoding step is also where memory usage becomes a major constraint on smaller GPUs.

Occupancy learning has a severe class imbalance problem because most voxels are free space. Focal loss~\cite{focal} is a standard tool to emphasize rare/hard examples, but in this project it also made memory usage worse in the backward pass.
Part of my work was deciding which imbalance approach could realistically run on a 12\,GB card (focal loss vs weighted cross entropy vs reducing queries / grid size).

\section{System / Model / Algorithm Design}
\subsection{Runtime environment}
All experiments and tooling in this repository assume a Python 3.7 environment. The conda specification \texttt{docs/occformer.yaml} pins the main stack to PyTorch 1.10.1 (CUDA 11.3.1), MMCV-Full 1.4.0, MMDetection 2.14.0, and MMDetection3D 0.17.1.
On the class cluster I primarily ran on a single Tesla P100 12\,GB (see the \texttt{nvidia-smi} header in \texttt{results/waymo\_full/waymo\_full\_15649.out}).

\begin{table}[h]
  \centering
  \begin{tabular}{ll}
    \toprule
    Component & Version (source) \\
    \midrule
    Python & 3.7.16 (\texttt{docs/occformer.yaml}) \\
    PyTorch & 1.10.1 + CUDA 11.3.1 (\texttt{docs/occformer.yaml}) \\
    MMCV-Full & 1.4.0 (\texttt{docs/occformer.yaml}) \\
    MMDetection & 2.14.0 (\texttt{docs/occformer.yaml}) \\
    MMDetection3D & 0.17.1 (\texttt{docs/occformer.yaml}) \\
    GPU (typical) & Tesla P100-PCIE-12GB (\texttt{results/waymo\_full/waymo\_full\_15649.out}) \\
    \bottomrule
  \end{tabular}
  \caption{Software/hardware stack used for this project.}
  \label{tab:env}
\end{table}

\subsection{Data: Waymo sensor exports + Occ3D-Waymo occupancy labels}
This project combines two sources of information that have to agree on indexing and geometry. The Occ3D-Waymo labels used here are voxelized at 0.4\,m resolution over a range of approximately $[-40, -40, -1, 40, 40, 5.4]$ meters, which corresponds to a 200$\times$200$\times$16 volume. The semantic labels are 0--14, and free space is stored as label 23 (summarized in \texttt{tools/data\_downloader/waymo/occ3d\_waymo.md}).

\textbf{Waymo sensor data (images + LiDAR).} In my workspace, Waymo Open Dataset TFRecords are stored under \texttt{data/waymo\_v1-3-1/waymo\_format/} with \texttt{training/}, \texttt{validation/}, and \texttt{testing/} subfolders. The preparation notes I followed are in \texttt{tools/data\_downloader/waymo/occ3d\_waymo.md} (download Waymo v1.3.1 from the Waymo site, then place TFRecords under \texttt{waymo\_format/}). MMDetection3D training expects a ``dataset directory'' rather than TFRecords, so I used the built-in MMDetection3D converter (\texttt{mmdetection3d/tools/create\_data.py waymo}) to export a KITTI-style layout. The helper script \texttt{scripts/sample\_waymo\_data.sh} shows the exact converter command for a small subset, and the full conversion log is saved as \texttt{create\_waymo\_data\_13356.log}.
The converter writes \texttt{data/waymo\_v1-3-1/kitti\_format/} containing \texttt{training/image\_0..4/}, \texttt{training/velodyne/}, \texttt{training/calib/}, and metadata PKLs such as \texttt{kitti\_infos\_train.pkl}, \texttt{kitti\_infos\_val.pkl}, and the augmentation database under \texttt{kitti\_gt\_database/}.

One practical issue I ran into is that the Waymo converter uses TensorFlow. When I launched it on a CPU node (\texttt{c1.hpc.coe}), TensorFlow printed warnings about missing \texttt{libcuda.so.1} and \texttt{/proc/driver/nvidia/version} in \texttt{create\_waymo\_data\_13356.log}. The job still completed on CPU and produced the KITTI-format output files, but it is an easy failure mode if you expect CUDA to be available everywhere.

\textbf{Occupancy ground truth (Occ3D-Waymo style).} The voxel ground truth is stored separately under \texttt{data/waymo\_v1-3-1/occ3d\_waymo/}. Each frame has an \texttt{.npz} file under \texttt{training/<scene>/<frame>\_04.npz} or \texttt{validation/<scene>/<frame>\_04.npz}. The metadata that indexes frames and cameras is stored in \texttt{waymo\_infos\_train.pkl} / \texttt{waymo\_infos\_val.pkl} and camera/pose pickles \texttt{cam\_infos.pkl} and \texttt{cam\_infos\_vali.pkl}. The same preparation document (\texttt{tools/data\_downloader/waymo/occ3d\_waymo.md}) includes the download link for the GT package and the expected directory layout.
Because metadata and local file layout can drift (missing files, different roots, partial downloads), I generated filtered info files using \texttt{tools/filter\_waymo\_infos.py}, which scans the KITTI-format directory, drops samples with missing sensor or occupancy files, and can optionally subsample. The configs in this repo point to \texttt{waymo\_infos\_train.filtered.pkl} and \texttt{waymo\_infos\_val.filtered.pkl}.

\subsection{Waymo dataset interface and model configuration}
The base Waymo configuration is \texttt{projects/configs/occformer\_waymo/waymo\_base.py}.
It uses the dataset class \texttt{CustomWaymoDataset\_T} from \texttt{projects/mmdet3d\_plugin/datasets/waymo\_temporal\_zlt.py} with five camera views.
At runtime the dataset joins KITTI-format sensor data (\texttt{data\_root=data/waymo\_v1-3-1/kitti\_format}) with Occ3D-Waymo metadata (\texttt{ann\_file=data/waymo\_v1-3-1/occ3d\_waymo/waymo\_infos\_*.pkl}).
For occupancy labels, the dataset first looks for an explicit occupancy path inside the info dict (keys such as \texttt{gt\_occ} / \texttt{occ\_path}). If no path is present, it falls back to a deterministic convention and constructs \texttt{.../occ3d\_waymo/<split>/<scene>/<frame>\_04.npz} directly from the integer \texttt{sample\_idx}.

On the model side, the config follows the original OccFormer pipeline: image backbone + neck, an image-to-voxel view transformer, a 3D encoder, and a Mask2Former-style occupancy head.
To support many experiments without duplicating configs, I added an experiment-preset layer in \texttt{projects/configs/occformer\_waymo/experiments.py} and a training entry point \texttt{tools/train\_waymo.py} that applies these overrides (learning rate, augmentation, backbone, query count, and loss function) and keeps outputs organized under \texttt{results/<exp\_name>/}.

\section{Implementation Details}
\subsection{Codebase structure and what I actually modified}
OccFormer itself is implemented under \texttt{projects/mmdet3d\_plugin/occformer/}. My Waymo work primarily lives in three places:
(1) dataset and pipeline code under \texttt{projects/mmdet3d\_plugin/datasets/}, (2) configs/presets under \texttt{projects/configs/occformer\_waymo/}, and (3) runnable scripts under \texttt{tools/} and \texttt{scripts/}.
The most important files for ``this repo works on Waymo'' are \texttt{projects/configs/occformer\_waymo/waymo\_base.py}, \texttt{projects/configs/occformer\_waymo/experiments.py}, \texttt{tools/train\_waymo.py}, and \texttt{tools/make\_waymo\_demo.py}.

\subsection{Debugging stories (problems, root causes, fixes)}
This project was dominated by integration bugs. Below are the issues that repeatedly blocked training/evaluation, the actual errors I saw, and the concrete fixes in the repo.

\paragraph{1) Registry errors (dataset/loss not found)}
MMDetection3D uses registries for datasets, heads, and losses, and a class is only registered if its module has been imported (so the decorator runs). Early on, experiments that enabled focal loss failed at build time. For example, \texttt{results/improved\_fast/logs/train\_15593.log} shows:
\begin{lstlisting}
KeyError: 'OccupancyFocalLoss is not in the models registry'
\end{lstlisting}
I fixed this by making sure the plugin imports its \texttt{models} module in \texttt{projects/mmdet3d\_plugin/\_\_init\_\_.py} so loss registration happens before config building. For visualization tools, I also import the plugin explicitly before calling \texttt{build\_dataset()} and \texttt{build\_model()} (see \texttt{tools/make\_waymo\_demo.py}).

\paragraph{2) Wrong image loader for Waymo (nuScenes-style \texttt{curr} dict)}
OccFormer was originally written for nuScenes and SemanticKITTI, and some pipelines assume nuScenes-style nested dictionaries. Waymo KITTI-format data is a flat dict. When the wrong loader was used, training crashed with a \texttt{KeyError: 'curr'} (summarized in \texttt{FIXES\_APPLIED.md}). The fix was to use the KITTI-style multi-view loader \texttt{LoadMultiViewImageFromFiles\_SemanticKitti} in \texttt{projects/configs/occformer\_waymo/waymo\_base.py}.

\paragraph{3) Camera/pose index mismatch (two views swapped)}
Occ3D-Waymo provides camera pose information in a fixed order, but for two views the pose index did not match the camera image folder naming in my export. If left unfixed, the model would use incorrect intrinsics/extrinsics for those images, which breaks the image-to-voxel view transformation.
I fixed this directly in \texttt{get\_data\_info()} in \texttt{projects/mmdet3d\_plugin/datasets/waymo\_temporal\_zlt.py} by swapping the image paths for indices 2 and 3 (mapping pose index 2 $\rightarrow$ \texttt{image\_3} and pose index 3 $\rightarrow$ \texttt{image\_2}). I mirrored the same swap in \texttt{tools/filter\_waymo\_infos.py} so the filtering step checks the correct camera files.

\paragraph{4) SLURM ``Invalid device id'' (hard-coded GPU IDs)}
Another early failure happened before the first iteration. Some configs assumed four GPUs (\texttt{gpu\_ids=[0,1,2,3]}), but SLURM often allocated a single GPU. The error is captured in \texttt{results/waymo\_full/waymo\_full\_15594.out}:
\begin{lstlisting}
AssertionError: Invalid device id
\end{lstlisting}
I fixed this in \texttt{tools/train\_waymo.py} by overriding \texttt{cfg.gpu\_ids} from \texttt{torch.cuda.device\_count()} at runtime. This made scripts robust regardless of how many GPUs the job was assigned.

\paragraph{5) Occupancy label convention mismatch (free-space label)}
One subtle issue was label conventions. In my Waymo occupancy GT, free space appears as label 23 in the raw voxel grid. The evaluation setup in this repo uses a 16-class occupancy label set where free space is class 15. If you do not remap 23 $\rightarrow$ 15, the model trains against an inconsistent label space and results are not meaningful. I implemented this remapping in the occupancy loading pipeline in \texttt{projects/mmdet3d\_plugin/datasets/pipelines/loading\_waymo\_occ.py} (and mirrored the same remap in \texttt{loading\_kitti\_occ.py} for consistency).

\paragraph{6) GT grid size mismatch (200$\times$200$\times$16 vs 256$\times$256$\times$32)}
Waymo occupancy GT volumes in \texttt{occ3d\_waymo} are commonly 200$\times$200$\times$16, but many OccFormer configs and visualization assumptions are 256$\times$256$\times$32. This mismatch shows up both in training (shape mismatch during loss) and in visualization (shape mismatch during concatenation). I handled this in two places:

First, \texttt{tools/train\_waymo.py} supports experiment presets with \texttt{use\_waymo\_loader=True}. When enabled, the script replaces \texttt{LoadSemKittiAnnotation} with \texttt{LoadWaymoOccAnnotation} so GT can be resized to a target size via nearest-neighbor. This makes the behavior explicit per experiment instead of silently changing GT for everything.

Second, \texttt{tools/make\_waymo\_demo.py} detects when GT and prediction shapes differ at runtime and resizes GT to the prediction grid before rendering. This directly addresses the type of crash I hit while generating videos.

\paragraph{7) CUDA out-of-memory during Mask2Former decoding (12\,GB GPU)}
The hardest blocker was GPU memory. Focal-loss experiments often crashed inside the Mask2Former-style occupancy head. A reproducible example is \texttt{results/improved\_fast/logs/train\_15655.log}:
\begin{lstlisting}
RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB ...
mask_pred = torch.einsum('bqc,bcxyz->bqxyz', mask_embed, mask_feature)
\end{lstlisting}
The underlying issue is that the query-based mask prediction allocates a large intermediate of shape \texttt{[B,Q,H,W,D]}. Reducing the number of queries reduces this tensor directly. Focal loss also increases backward memory compared to plain cross entropy, and fragmentation makes the final allocation unstable even when there appears to be ``some'' free memory.

My mitigation steps are implemented and documented in the repo: I rewrote focal loss to avoid storing a full softmax tensor (\texttt{projects/mmdet3d\_plugin/models/losses/focal\_loss\_balanced.py}), reduced Mask2Former query count in memory-heavy presets (\texttt{projects/configs/occformer\_waymo/experiments.py}), and added allocator/caching hygiene in \texttt{tools/train\_waymo.py}. The notes \texttt{MEMORY\_FIX\_REPORT.md} and \texttt{FINAL\_SOLUTION.md} capture the reasoning and expected savings.

\paragraph{8) SLURM script failure before Python starts (\texttt{\$PYTHONPATH} unbound)}
Some failures were pure shell issues. One evaluation job died immediately because the SLURM script ran with \texttt{set -u} and treated \texttt{\$PYTHONPATH} as an unbound variable. The error is recorded in \texttt{results/waymo\_eval/waymo\_eval\_15330.err}:
\begin{lstlisting}
/var/spool/slurmd/job15330/slurm_script: line 31: PYTHONPATH: unbound variable
\end{lstlisting}
I fixed this by exporting \texttt{PYTHONPATH} safely as \texttt{\$\{PYTHONPATH:-\}} in batch scripts (for example \texttt{scripts/eval\_waymo\_experiments.sh} and \texttt{scripts/run\_experiment.sh}).

\section{Task Distribution \& Contributions}
This was an individual project. I handled the Waymo dataset integration, conversion/metadata filtering, training/evaluation runs, debugging, and the tooling for plots and video demo.

\section{Evaluation \& Testing Results}
\subsection{Metrics and evaluation notes}
I use semantic mIoU over occupancy classes as the primary metric. Per-class IoU tables are read from the printed summary at the end of each \texttt{results/*/logs/evaluate.log}.
One confusing detail is that the progress messages in these logs still say ``Evaluating nuScenes occupancy'' even for Waymo; this string comes from reused evaluation code.

Another detail that matters for reproducibility: my evaluation logs print a correct per-class IoU table, but then end with:
\begin{lstlisting}
Evaluation skipped: required keys (count_matrix/scene_id/frame_id) missing.
{'waymo_SSC_mIoU': 0.0}
\end{lstlisting}
This appears in \texttt{results/baseline\_fast/logs/evaluate.log} and other runs. For this report I use the printed per-class IoU table (which includes mIoU) as the quantitative result, because the returned metric dict is clearly inconsistent.

\subsection{Experiment summary (from saved logs)}
Table~\ref{tab:miou} summarizes the runs that produced a valid IoU table in \texttt{evaluate.log}. The runs are configured through \texttt{projects/configs/occformer\_waymo/experiments.py} and trained with \texttt{tools/train\_waymo.py}.

\begin{table}[h]
  \centering
  \begin{tabular}{lr}
    \toprule
    Experiment & mIoU (\%) \\
    \midrule
    baseline\_fast & 13.41 \\
    strong\_aug & 12.43 \\
    sgd & 11.97 \\
    lr\_5e4 & 7.29 \\
    \bottomrule
  \end{tabular}
  \caption{Waymo val mIoU parsed from the printed IoU tables in \texttt{results/*/logs/evaluate.log}.}
  \label{tab:miou}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/miou_by_experiment.png}
  \caption{mIoU across experiments (generated by \texttt{final\_report/make\_figures.py} from \texttt{results/*/logs/evaluate.log}).}
  \label{fig:miou}
\end{figure}

\subsection{Per-class IoU (baseline\_fast)}
Table~\ref{tab:perclass} reports the per-class IoU table printed at the end of \texttt{results/baseline\_fast/logs/evaluate.log}. Free space and road are learned reasonably, while several rare classes stay near 0 IoU, which matches the imbalance issue I observed during debugging.

\begin{table}[h]
  \centering
  \begin{tabular}{lrlr}
    \toprule
    Class & IoU & Class & IoU \\
    \midrule
    general\_object & 0.00 & bicycle & 1.04 \\
    vehicle & 18.69 & motorcycle & 0.00 \\
    pedestrian & 9.61 & building & 8.08 \\
    sign & 1.79 & vegetation & 7.78 \\
    cyclist & 7.22 & tree\_trunk & 3.52 \\
    traffic\_light & 0.68 & road & 35.56 \\
    pole & 4.34 & walkable & 20.10 \\
    construction\_cone & 4.05 & free & 92.02 \\
    \bottomrule
  \end{tabular}
  \caption{Per-class IoU for \texttt{baseline\_fast} from \texttt{results/baseline\_fast/logs/evaluate.log}.}
  \label{tab:perclass}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/per_class_baseline_fast.png}
  \caption{Per-class IoU for \texttt{baseline\_fast}.}
  \label{fig:perclass}
\end{figure}

\subsection{Correctness checks and reproducibility}
Before launching long SLURM runs, I used small checks to catch obvious integration errors. \texttt{scripts/quick\_test.sh} imports the plugin, checks registry registration, and instantiates the occupancy loader to confirm label remapping and optional resizing. I also used \texttt{waymo\_debug.ipynb} to load a batch and inspect GT/output tensor shapes interactively.

For a typical interactive session on the cluster:
\begin{lstlisting}
srun -p gpuqs --pty /bin/bash
conda activate occformer307
cd /fs/atipa/home/018219422/OccFormerWithWaymoData
export PYTHONPATH=$(pwd):$PYTHONPATH
\end{lstlisting}

Training with SLURM wrappers:
\begin{lstlisting}
sbatch scripts/run_experiment.sh baseline_fast
\end{lstlisting}

For focal-loss experiments, I used the same wrapper but different experiment presets. \texttt{improved\_fast} is the ``heavier'' focal-loss configuration (resizes GT to 256$\times$256$\times$32), while \texttt{improved\_small\_grid} is the more memory-friendly option (keeps 200$\times$200$\times$16, uses EfficientNet-B4, and \texttt{samples\_per\_gpu=1}):
\begin{lstlisting}
# heavier (more GPU memory)
sbatch scripts/run_full_experiment.sh improved_fast

# lighter (most likely to fit on 12GB)
sbatch scripts/run_full_experiment.sh improved_small_grid
\end{lstlisting}

Evaluation:
\begin{lstlisting}
python tools/test.py projects/configs/occformer_waymo/waymo_base.py \
  results/baseline_fast/model/latest.pth --eval mIoU --launcher none
\end{lstlisting}

\subsection{Video demo}
I added \texttt{tools/make\_waymo\_demo.py} to generate an OccFormer-style video for Waymo that shows a camera mosaic and BEV occupancy slices from both prediction and GT:
\begin{lstlisting}
python tools/make_waymo_demo.py \
  projects/configs/occformer_waymo/waymo_base.py \
  results/baseline_fast/model/latest.pth \
  --scene-id 0 --max-frames 120 --slice-idx 8 --fps 5 \
  --out results/waymo_demo/baseline_fast_scene0.mp4
\end{lstlisting}
The script imports the plugin so custom datasets are registered, unwraps MMCV \texttt{DataContainer} objects for visualization, and resizes GT when its grid shape does not match the model output.

\section{Limitations and Future Work}
My best saved mIoU (13.41) is lower than I expected for a clean Waymo fine-tune, and several ``improved'' ideas did not complete cleanly under the 12\,GB constraint. The two biggest practical limitations were GPU memory (focal loss + Mask2Former decoding often OOM) and data fraction (I trained on 10\% via \texttt{load\_interval=10} to keep runtimes practical).

If I had more time or more GPU headroom, I would (1) rerun focal-loss variants on a larger GPU starting from \texttt{improved\_small\_grid}, (2) fix the evaluation output path so \texttt{waymo\_SSC\_mIoU} is logged consistently (the current ``count\_matrix missing'' issue), and (3) increase the data fraction toward full training once the memory baseline is stable.

\section{References}
\begin{thebibliography}{9}
\bibitem{occformer}
Y. Zhang, Z. Zhu, and D. Du, ``OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction,'' arXiv:2304.05316, 2023.

\bibitem{focal}
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll{\'a}r, ``Focal Loss for Dense Object Detection,'' ICCV 2017.

\bibitem{waymo}
Waymo Open Dataset, \url{https://waymo.com/open/}.

\bibitem{mmdet3d}
OpenMMLab MMDetection3D, \url{https://github.com/open-mmlab/mmdetection3d}.

\bibitem{occ3d}
Tsinghua MARS Lab Occ3D (Waymo occupancy labels), \url{https://github.com/Tsinghua-MARS-Lab/Occ3D}.
\end{thebibliography}

\end{document}
